import os
import sys
from dotenv import load_dotenv
from langchain_community.llms import Ollama
import streamlit as st

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Ollama ì„¤ì •ì„ ìœ„í•œ í™˜ê²½ ë³€ìˆ˜ë“¤
# .env íŒŒì¼ì— ë‹¤ìŒ ë³€ìˆ˜ë“¤ì„ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ì„ íƒì‚¬í•­):
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=llama2:7b

# Chat ëª¨ë¸ (Ollama ì‚¬ìš©)
@st.cache_resource
def get_ollama_model():
    base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    model_name = os.getenv("OLLAMA_MODEL", "llama2:7b")
    
    return Ollama(
        model=model_name,
        base_url=base_url,
        temperature=0.7
    )

def chat_with_ai(user_input):
    """ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ì„œ AI ì‘ë‹µì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
    try:
        ollama_model = get_ollama_model()
        response = ollama_model.invoke(user_input)
        return response
    except Exception as e:
        return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\n\nğŸ’¡ Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”:\n- ollama serve\n- ollama pull {os.getenv('OLLAMA_MODEL', 'llama2:7b')}"

# Streamlit ì›¹ ì•±
st.title("ğŸ¦™ Ollama ëŒ€í™” ì•±")
st.markdown(f"**ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸:** {os.getenv('OLLAMA_MODEL', 'llama2:7b')}")
st.markdown(f"**Ollama ì„œë²„:** {os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')}")

# ì—°ê²° ìƒíƒœ í™•ì¸
with st.sidebar:
    st.header("ğŸ”§ Ollama ì„¤ì •")
    
    # ëª¨ë¸ ì„ íƒ
    available_models = ["llama2:7b", "llama2:13b", "codellama:7b", "mistral:7b", "gemma:7b"]
    selected_model = st.selectbox(
        "ëª¨ë¸ ì„ íƒ",
        available_models,
        index=0 if os.getenv('OLLAMA_MODEL') is None else available_models.index(os.getenv('OLLAMA_MODEL', 'llama2:7b'))
    )
    
    # í™˜ê²½ ë³€ìˆ˜ ì—…ë°ì´íŠ¸ (ì„¸ì…˜ ìƒíƒœë¡œ)
    if "selected_model" not in st.session_state:
        st.session_state.selected_model = selected_model
    
    if st.session_state.selected_model != selected_model:
        st.session_state.selected_model = selected_model
        os.environ["OLLAMA_MODEL"] = selected_model
        st.cache_resource.clear()  # ìºì‹œ í´ë¦¬ì–´
    
    # ì—°ê²° í…ŒìŠ¤íŠ¸ ë²„íŠ¼
    if st.button("ğŸ” ì—°ê²° í…ŒìŠ¤íŠ¸"):
        with st.spinner("Ollama ì—°ê²° í™•ì¸ ì¤‘..."):
            test_response = chat_with_ai("Hello")
            if "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤" in test_response:
                st.error("âŒ Ollama ì—°ê²° ì‹¤íŒ¨")
                st.code(test_response)
            else:
                st.success("âœ… Ollama ì—°ê²° ì„±ê³µ")
    
    st.markdown("---")
    st.header("ğŸ“‹ ì‚¬ìš©ë²•")
    st.markdown("""
    **Ollama ì„¤ì¹˜ ë° ì‹¤í–‰:**
    ```bash
    # 1. Ollama ì„¤ì¹˜
    curl -fsSL https://ollama.ai/install.sh | sh
    
    # 2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
    ollama pull llama2:7b
    
    # 3. ì„œë²„ ì‹¤í–‰
    ollama serve
    ```
    """)
    
    if st.button("ğŸ—‘ï¸ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.rerun()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ëŒ€í™” ê¸°ë¡ ì €ì¥ìš©)
if "messages" not in st.session_state:
    st.session_state.messages = []

# ëŒ€í™” ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # AI ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        with st.spinner("ğŸ¦™ Llamaê°€ ìƒê° ì¤‘..."):
            # í˜„ì¬ ì„ íƒëœ ëª¨ë¸ ì‚¬ìš©
            os.environ["OLLAMA_MODEL"] = st.session_state.get("selected_model", "llama2:7b")
            response = chat_with_ai(prompt)
        st.markdown(response)
    
    # AI ì‘ë‹µ ì €ì¥
    st.session_state.messages.append({"role": "assistant", "content": response})