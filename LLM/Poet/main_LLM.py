# ğŸ“š ë˜‘ë˜‘í•œ PDF ê²€ìƒ‰ í”„ë¡œê·¸ë¨
# ì´ í”„ë¡œê·¸ë¨ì€ PDF íŒŒì¼ì„ ì½ê³ , AIê°€ ë„ì›€ì„ ì¤˜ì„œ ë‚´ìš©ì„ ì°¾ì•„ì£¼ëŠ” í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤!

# í•„ìš”í•œ ë„êµ¬ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤ (ë§ˆì¹˜ ì—°í•„, ì§€ìš°ê°œë¥¼ ì¤€ë¹„í•˜ëŠ” ê²ƒì²˜ëŸ¼!)
import os          # ì»´í“¨í„° íŒŒì¼ì„ ë‹¤ë£¨ëŠ” ë„êµ¬
import time        # ì‹œê°„ì„ ì¬ëŠ” ë„êµ¬ (ìŠ¤í†±ì›Œì¹˜ ê°™ì€ ê²ƒ)
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI        # AIê°€ ê¸€ì„ ì´í•´í•˜ê³  ëŒ€í™”í•˜ëŠ” ë„êµ¬
from langchain_community.document_loaders import PyPDFLoader  # PDFë¥¼ ì½ëŠ” ë„êµ¬
from langchain_text_splitters import CharacterTextSplitter    # ê¸´ ê¸€ì„ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ë„êµ¬
from langchain_community.vectorstores import FAISS         # ë©”ëª¨ë¦¬ì—ì„œ ê²€ìƒ‰í•  ìˆ˜ ìˆê²Œ ì €ì¥í•˜ëŠ” ë„êµ¬
from langchain.chains import RetrievalQA                   # ê²€ìƒ‰ê³¼ ì§ˆë‹µì„ ì—°ê²°í•˜ëŠ” ë„êµ¬
from langchain.memory import ConversationBufferMemory      # ëŒ€í™” ê¸°ë¡ì„ ê¸°ì–µí•˜ëŠ” ë„êµ¬
from langchain_core.messages import HumanMessage, AIMessage       # ì‚¬ëŒê³¼ AI ë©”ì‹œì§€ í˜•ì‹
from dotenv import load_dotenv                           # ë¹„ë°€ë²ˆí˜¸ ê°™ì€ ì„¤ì •ì„ ì½ëŠ” ë„êµ¬

# ğŸ” ë¹„ë°€ ì„¤ì •ë“¤ì„ ì½ì–´ì˜µë‹ˆë‹¤ (ë¶€ëª¨ë‹˜ì´ ìˆ¨ê²¨ë‘” ì„¤ì • íŒŒì¼)
load_dotenv()

def main():
    # ğŸš€ í”„ë¡œê·¸ë¨ì´ ì‹œì‘ë©ë‹ˆë‹¤!
    try:
        # ğŸ“– 1ë‹¨ê³„: PDF íŒŒì¼ì„ ì°¾ì•„ì„œ ì½ê¸°
        pdf_path = "testpdf.pdf"  # ì½ì„ PDF íŒŒì¼ ì´ë¦„
        
        # íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê¸° (ì±…ì´ ì±…ìƒì— ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ê²ƒì²˜ëŸ¼!)
        if not os.path.exists(pdf_path):
            print(f"ì•—! PDF íŒŒì¼ '{pdf_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”!")
            print("ğŸ’¡ ë¨¼ì € 'python create_test_pdf.py'ë¥¼ ì‹¤í–‰í•´ì„œ í…ŒìŠ¤íŠ¸ íŒŒì¼ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”!")
            return  # íŒŒì¼ì´ ì—†ìœ¼ë©´ í”„ë¡œê·¸ë¨ ëë‚´ê¸°
            
        print(f">> PDF íŒŒì¼ì„ ì½ëŠ” ì¤‘: {pdf_path}")
        loader = PyPDFLoader(pdf_path)  # PDF ì½ëŠ” ë„êµ¬ ì¤€ë¹„
        pages = loader.load()           # PDFì˜ ëª¨ë“  í˜ì´ì§€ ì½ê¸°
        print(f">> ì´ {len(pages)}í˜ì´ì§€ë¥¼ ì½ì—ˆì–´ìš”!")

        # âœ‚ï¸ 2ë‹¨ê³„: ê¸´ ê¸€ì„ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ê¸° (í° í”¼ìë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ìë¥´ëŠ” ê²ƒì²˜ëŸ¼!)
        text_splitter = CharacterTextSplitter(
            separator="\n\n",      # ë¬¸ë‹¨ì´ ë°”ë€ŒëŠ” ê³³ì—ì„œ ë‚˜ëˆ„ê¸°
            chunk_size=10000,       # í•œ ì¡°ê°ì€ 1000ê¸€ì ì •ë„ë¡œ
            chunk_overlap=2000,     # ì¡°ê°ë“¤ì´ 200ê¸€ìì”© ê²¹ì¹˜ê²Œ (ë‚´ìš©ì´ ëŠì–´ì§€ì§€ ì•Šê²Œ!)
            length_function=len,   # ê¸€ì ìˆ˜ ì„¸ëŠ” ë°©ë²•
            is_separator_regex=False,  # ë³µì¡í•œ ê·œì¹™ ì‚¬ìš© ì•ˆ í•¨
        )
        texts = text_splitter.split_documents(pages)  # ì‹¤ì œë¡œ ë‚˜ëˆ„ê¸° ì‹¤í–‰!
        print(f">> ì´ {len(texts)}ê°œì˜ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ì—ˆì–´ìš”!")
        
        # ğŸ“Š Rate limitì„ í”¼í•˜ê¸° ìœ„í•´ í…ìŠ¤íŠ¸ ì¡°ê° ìˆ˜ ì œí•œ
        max_chunks = 50  # ìµœëŒ€ 50ê°œ ì¡°ê°ë§Œ ì‚¬ìš© (API í˜¸ì¶œ ì œí•œ ë•Œë¬¸ì—)
        if len(texts) > max_chunks:
            print(f"âš ï¸  í…ìŠ¤íŠ¸ ì¡°ê°ì´ ë„ˆë¬´ ë§ì•„ìš”! {len(texts)}ê°œ â†’ {max_chunks}ê°œë¡œ ì œí•œí•©ë‹ˆë‹¤.")
            texts = texts[:max_chunks]
                
        # ğŸ§  3ë‹¨ê³„: AIê°€ ê¸€ì„ ì´í•´í•  ìˆ˜ ìˆê²Œ ë„ì™€ì£¼ëŠ” ë„êµ¬ ì¤€ë¹„í•˜ê¸°
        print(">> AIê°€ ê¸€ì„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì¤€ë¹„ ì¤‘...")
        embeddings_model = AzureOpenAIEmbeddings(
            # ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ AI ì„œë¹„ìŠ¤ ì„¤ì •ë“¤ (ì–´ë¥¸ë“¤ì´ ì„¤ì •í•´ë‘” ê²ƒ)
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            azure_deployment=os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT"),
            api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
            api_key=os.getenv("AZURE_OPENAI_API_KEY"),
        )

        # ğŸ¤– 4ë‹¨ê³„: Chatìš© AI ëª¨ë¸ ì¤€ë¹„í•˜ê¸° (ëŒ€í™”í•  ìˆ˜ ìˆëŠ” ë˜‘ë˜‘í•œ AI!)
        print(">> ëŒ€í™”ìš© AI ëª¨ë¸ ì¤€ë¹„ ì¤‘...")
        chat_model = AzureChatOpenAI(
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            azure_deployment=os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT"),  # GPT ëª¨ë¸ìš© ë°°í¬ ì´ë¦„
            api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
            api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            temperature=0.1,  # ë‹µë³€ì´ ì¼ì •í•˜ê²Œ ë‚˜ì˜¤ë„ë¡ (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¼ì •í•¨)
        )

        # ğŸ—ƒï¸ 5ë‹¨ê³„: ë©”ëª¨ë¦¬ì— ê²€ìƒ‰ ê°€ëŠ¥í•œ ë°ì´í„°ë² ì´ìŠ¤ ë§Œë“¤ê¸° (ë¹ ë¥´ê²Œ ê²€ìƒ‰í•  ìˆ˜ ìˆì–´ìš”!)
        print(">> ë©”ëª¨ë¦¬ì— ê²€ìƒ‰ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë§Œë“œëŠ” ì¤‘... (ì¡°ê¸ˆ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”!)")
        print(f"   ğŸ“Š ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ ì¡°ê°: {len(texts)}ê°œ")
        print("   â° Rate limit ë•Œë¬¸ì— ì²œì²œíˆ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
        
        # Rate limitì„ í”¼í•˜ê¸° ìœ„í•´ ë°°ì¹˜ë¡œ ì²˜ë¦¬
        max_retries = 3
        for attempt in range(max_retries):
            try:
                vectorstore = FAISS.from_documents(
                    documents=texts,              # ë‚˜ëˆˆ ê¸€ ì¡°ê°ë“¤ì„
                    embedding=embeddings_model   # AIê°€ ì´í•´í•  ìˆ˜ ìˆê²Œ ë³€í™˜í•´ì„œ ë©”ëª¨ë¦¬ì— ì €ì¥
                )
                print(f">> ì§±! {len(texts)}ê°œì˜ ê¸€ ì¡°ê°ìœ¼ë¡œ ë©”ëª¨ë¦¬ ê²€ìƒ‰ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë§Œë“¤ì—ˆì–´ìš”!")
                break  # ì„±ê³µí•˜ë©´ ë£¨í”„ ì¢…ë£Œ
            except Exception as embed_error:
                if "429" in str(embed_error) or "rate limit" in str(embed_error).lower():
                    if attempt < max_retries - 1:
                        wait_time = 60 * (attempt + 1)  # 1ë¶„, 2ë¶„, 3ë¶„ ëŒ€ê¸°
                        print(f"   â³ Rate limit ì˜¤ë¥˜! {wait_time}ì´ˆ í›„ ì¬ì‹œë„... ({attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                    else:
                        print(f"   âŒ {max_retries}ë²ˆ ì‹œë„í–ˆì§€ë§Œ ì—¬ì „íˆ Rate limit ì˜¤ë¥˜ì…ë‹ˆë‹¤.")
                        print("   ğŸ’¡ í•´ê²°ë°©ë²•:")
                        print("   1. 60ë¶„ í›„ ë‹¤ì‹œ ì‹œë„")
                        print("   2. ë” ì‘ì€ PDF íŒŒì¼ ì‚¬ìš©")
                        print("   3. Azure OpenAI ìš”ê¸ˆ ê³„ì¸µ ì—…ê·¸ë ˆì´ë“œ")
                        raise embed_error
                else:
                    raise embed_error

        # ğŸ”— 6ë‹¨ê³„: ê²€ìƒ‰ê³¼ ì§ˆë‹µì„ ì—°ê²°í•˜ëŠ” ì²´ì¸ ë§Œë“¤ê¸°
        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 5}  # ê°€ì¥ ë¹„ìŠ·í•œ 5ê°œ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        )
        
        # ğŸ’¬ ëŒ€í™” ê¸°ë¡ì„ ê¸°ì–µí•  ìˆ˜ ìˆëŠ” ë©”ëª¨ë¦¬ ë§Œë“¤ê¸°
        memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            output_key="answer"
        )

        # ğŸ¤ ê²€ìƒ‰ê³¼ ëŒ€í™”ë¥¼ ì—°ê²°í•˜ëŠ” ì²´ì¸ ë§Œë“¤ê¸°
        qa_chain = RetrievalQA.from_chain_type(
            llm=chat_model,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True
        )

        # ğŸ” 7ë‹¨ê³„: ì´ì œ Chat í˜•íƒœë¡œ ì§ˆë¬¸ë‹µë³€ì„ ì‹œì‘í•©ë‹ˆë‹¤!
        print("\n" + "="*50)
        print("ğŸ‰ ë˜‘ë˜‘í•œ PDF Chat AI ì™„ì„±! ì´ì œ ëŒ€í™”í•  ìˆ˜ ìˆì–´ìš”!")
        print("âŒ ëë‚´ê³  ì‹¶ìœ¼ë©´: 'quit' ë˜ëŠ” 'exit' ì…ë ¥")
        print("ğŸ“œ ëŒ€í™” ê¸°ë¡ ë³´ê¸°: 'history' ì…ë ¥")
        print("ğŸ”„ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”: 'clear' ì…ë ¥")
        print("="*50)
        
        chat_history = []  # ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        
        # ğŸ’¬ Chat ë£¨í”„: ì‚¬ìš©ìì™€ AIê°€ ê³„ì† ëŒ€í™”í•  ìˆ˜ ìˆê²Œ í•˜ê¸°
        while True:  # ë¬´í•œ ë°˜ë³µ (ì‚¬ìš©ìê°€ ê·¸ë§Œí•˜ê² ë‹¤ê³  í•  ë•Œê¹Œì§€)
            try:
                # ì‚¬ìš©ìì—ê²Œ ì§ˆë¬¸ ë¬¼ì–´ë³´ê¸°
                user_input = input("\nï¿½ AIì—ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”: ").strip()
                
                # ì‚¬ìš©ìê°€ ë‚˜ê°€ê³  ì‹¶ì–´í•˜ëŠ”ì§€ í™•ì¸í•˜ê¸°
                if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'ë‚˜ê°€ê¸°']:
                    # ì§€ê¸ˆê¹Œì§€ í•œ ëŒ€í™”ë“¤ì„ ìš”ì•½í•´ì„œ ë³´ì—¬ì£¼ê¸°
                    if chat_history:
                        print(f"\nğŸ“Š ì˜¤ëŠ˜ ëŒ€í™” ìš”ì•½:")
                        print(f"   â€¢ ì´ ëŒ€í™” íšŸìˆ˜: {len(chat_history)}ë²ˆ")
                        avg_response_time = sum(chat['response_time'] for chat in chat_history) / len(chat_history)
                        print(f"   â€¢ í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_response_time:.3f}ì´ˆ")
                    print("ğŸ‘‹ ëŒ€í™”ë¥¼ ë§ˆì¹ ê²Œìš”. ì•ˆë…•!")
                    break  # ë°˜ë³µë¬¸ì—ì„œ ë‚˜ê°€ê¸°
                
                # ëŒ€í™” ê¸°ë¡ ë³´ê¸°
                if user_input.lower() == 'history':
                    if chat_history:
                        print("\nğŸ“œ ëŒ€í™” ê¸°ë¡:")
                        print("-" * 50)
                        for i, chat in enumerate(chat_history, 1):
                            print(f"{i}. ğŸ‘¤ ì§ˆë¬¸: {chat['question']}")
                            print(f"   ğŸ¤– ë‹µë³€: {chat['answer'][:100]}...")
                            print(f"   â° ì‘ë‹µì‹œê°„: {chat['response_time']:.3f}ì´ˆ")
                            print("-" * 30)
                    else:
                        print("ğŸ“ ì•„ì§ ëŒ€í™” ê¸°ë¡ì´ ì—†ì–´ìš”!")
                    continue
                
                # ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
                if user_input.lower() == 'clear':
                    chat_history.clear()
                    memory.clear()
                    print("ğŸ”„ ëŒ€í™” ê¸°ë¡ì„ ì´ˆê¸°í™”í–ˆì–´ìš”!")
                    continue
                
                # ì•„ë¬´ê²ƒë„ ì…ë ¥í•˜ì§€ ì•Šì•˜ì„ ë•Œ
                if not user_input:
                    print("â“ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”!")
                    continue  # ë‹¤ì‹œ ì§ˆë¬¸ ë¬¼ì–´ë³´ê¸°
                
                print(f"\nğŸ¤– AIê°€ ë‹µë³€ì„ ìƒê°í•˜ëŠ” ì¤‘... ì ê¹ë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”!")
                response_start_time = time.time()  # ì‘ë‹µ ì‹œì‘ ì‹œê°„ ê¸°ë¡
                
                # ğŸ¤– AIì—ê²Œ ì§ˆë¬¸í•˜ê³  ë‹µë³€ ë°›ê¸°
                result = qa_chain.invoke({"query": user_input})
             
                response_end_time = time.time()    # ì‘ë‹µ ëë‚œ ì‹œê°„ ê¸°ë¡
                response_time = response_end_time - response_start_time  # ì´ ê±¸ë¦° ì‹œê°„ ê³„ì‚°
                
                # ğŸ“‹ AI ë‹µë³€ê³¼ ì¶œì²˜ ë¬¸ì„œ ë³´ì—¬ì£¼ê¸° (í‚¤ ì´ë¦„ í™•ì¸í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì ‘ê·¼)
                print(f"ğŸ” ë””ë²„ê·¸: ê²°ê³¼ í‚¤ë“¤ = {list(result.keys())}")  # ë””ë²„ê·¸ìš©
                
                # result ë”•ì…”ë„ˆë¦¬ì—ì„œ ì ì ˆí•œ í‚¤ ì°¾ê¸°
                if "result" in result:
                    answer = result["result"]
                elif "answer" in result:
                    answer = result["answer"]
                else:
                    # ì²« ë²ˆì§¸ ê°’ì„ ë‹µë³€ìœ¼ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜ ë¬¸ìì—´ë¡œ ë³€í™˜
                    answer = str(result.get(list(result.keys())[0], "ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."))
                
                source_docs = result.get("source_documents", [])
                
                print(f"\nğŸ¤– AI ë‹µë³€:")
                print("="*50)
                print(answer)
                print("="*50)
                
                # ğŸ“š ì°¸ê³ í•œ ë¬¸ì„œë“¤ ë³´ì—¬ì£¼ê¸°
                if source_docs:
                    print(f"\nğŸ“š ì°¸ê³ í•œ ë¬¸ì„œë“¤ ({len(source_docs)}ê°œ):")
                    for i, doc in enumerate(source_docs, 1):
                        print(f"\nğŸ“„ {i}ë²ˆì§¸ ì°¸ê³  ë¬¸ì„œ:")
                        print(f"    ë‚´ìš©: {doc.page_content[:200]}...")
                        if doc.metadata:
                            page_info = doc.metadata.get('page', 'Unknown')
                            source = doc.metadata.get('source', 'Unknown')
                            print(f"   ğŸ“ ìœ„ì¹˜: {page_info}í˜ì´ì§€ | íŒŒì¼: {source}")
                
                #  ì‘ë‹µ ì„±ëŠ¥ ë³´ì—¬ì£¼ê¸°
                print(f"\nâ° ì‘ë‹µ ì‹œê°„: {response_time:.3f}ì´ˆ")
                

                # ğŸ“ ì´ë²ˆ ëŒ€í™” ê¸°ë¡ ì €ì¥í•˜ê¸°
                chat_history.append({
                    'question': user_input,
                    'answer': answer,
                    'response_time': response_time,
                    'source_count': len(source_docs)
                })
                
                # ğŸ’­ ë©”ëª¨ë¦¬ì—ë„ ëŒ€í™” ì¶”ê°€ (ìˆ˜ë™ìœ¼ë¡œ)
                memory.chat_memory.add_user_message(user_input)
                memory.chat_memory.add_ai_message(answer)
                        
            except KeyboardInterrupt:  # Ctrl+Cë¥¼ ëˆŒë €ì„ ë•Œ
                print("\n\nğŸ‘‹ ëŒ€í™”ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤. ì•ˆë…•!")
                break
            except Exception as chat_error:  # ë­”ê°€ ì˜ëª»ëì„ ë•Œ
                print(f"ğŸ˜“ ëŒ€í™”í•˜ë‹¤ê°€ ë¬¸ì œê°€ ìƒê²¼ì–´ìš”: {chat_error}")
                print("ğŸ’¡ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”!")
            
    except Exception as e:  # í”„ë¡œê·¸ë¨ ì „ì²´ì— ë¬¸ì œê°€ ìƒê²¼ì„ ë•Œ
        print(f"ğŸš¨ í”„ë¡œê·¸ë¨ì— ë¬¸ì œê°€ ìƒê²¼ì–´ìš”: {str(e)}")
        print("ğŸ”§ ì–´ë¥¸ì—ê²Œ ë„ì›€ì„ ìš”ì²­í•˜ì„¸ìš”!")

if __name__ == "__main__":
    print("ğŸš€ ë˜‘ë˜‘í•œ PDF Chat AI í”„ë¡œê·¸ë¨ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    print("ğŸ“– PDF íŒŒì¼ì„ ì½ê³  AIì™€ ëŒ€í™”í•˜ë©´ì„œ ì›í•˜ëŠ” ë‚´ìš©ì„ ì°¾ì•„ë“œë ¤ìš”!")
    main()  # ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰í•˜ê¸°
    print("âœ¨ í”„ë¡œê·¸ë¨ì´ ëë‚¬ì–´ìš”. ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤!") 
