import streamlit as st
import os
import time
import tempfile
from datetime import datetime
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_chroma import Chroma
from langchain.memory import ConversationBufferWindowMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.schema import HumanMessage, SystemMessage
from dotenv import load_dotenv
import re

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸ¤– PDF Assistant with LangChain",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

class PDFAssistant:
    """PDF ê²€ìƒ‰ ì „ìš© AI ì–´ì‹œìŠ¤í„´íŠ¸ (Streamlitìš©)"""
    
    def __init__(self):
        """Assistant ì´ˆê¸°í™”"""
        
        # Azure OpenAI ì„¤ì • (Content Filter ìµœì í™”)
        self.llm = AzureChatOpenAI(
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            azure_deployment=os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT", "gpt-4"),
            openai_api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview"),
            api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            temperature=0.1,  # ë” ë‚®ì€ temperatureë¡œ ì•ˆì „ì„± ì¦ëŒ€
            max_tokens=800,   # ë” ì§§ì€ ì‘ë‹µìœ¼ë¡œ í•„í„° ìš°íšŒ
            top_p=0.7,        # ë” ë³´ìˆ˜ì ì¸ í† í° ì„ íƒ
        )
        
        # ì„ë² ë”© ëª¨ë¸ ì„¤ì •
        self.embeddings = AzureOpenAIEmbeddings(
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            azure_deployment=os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT"),
            openai_api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
            api_key=os.getenv("AZURE_OPENAI_API_KEY"),
        )
        
        # ì œí•œëœ ëŒ€í™” ê¸°ì–µ ì¥ì¹˜ (ìµœê·¼ 3ê°œë§Œ ê¸°ì–µ)
        self.memory = ConversationBufferWindowMemory(
            k=3,  # ìµœê·¼ 3ê°œ ëŒ€í™”ë§Œ ê¸°ì–µ
            memory_key="chat_history",
            return_messages=True,
            output_key='answer'
        )
        
        # ë²¡í„° ì €ì¥ì†Œì™€ ëŒ€í™” ì²´ì¸
        self.vectorstore = None
        self.conversation_chain = None
    
    def load_pdf_knowledge(self, uploaded_file):
        """ì—…ë¡œë“œëœ PDFë¥¼ Assistantì˜ ì§€ì‹ìœ¼ë¡œ ë³€í™˜"""
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_path = tmp_file.name
        
        try:
            # 1. PDF ë¡œë“œ
            loader = PyPDFLoader(tmp_path)
            pages = loader.load()
            
            # 2. í…ìŠ¤íŠ¸ ë¶„í• 
            text_splitter = CharacterTextSplitter(
                separator="\n\n",
                chunk_size=1000,
                chunk_overlap=200,
                length_function=len,
            )
            texts = text_splitter.split_documents(pages)
            
            # 3. ë²¡í„° ì €ì¥ì†Œ ìƒì„±
            self.vectorstore = Chroma.from_documents(
                documents=texts,
                embedding=self.embeddings,
                persist_directory="./chroma_db_streamlit"
            )
            
            # 4. ëŒ€í™”í˜• ê²€ìƒ‰ ì²´ì¸ ìƒì„±
            from langchain.prompts import PromptTemplate
            
            # ì•ˆì „í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„± (ê°„ì†Œí™”)
            safe_template = """ë‹¹ì‹ ì€ PDF ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
            
ë¬¸ì„œ ë‚´ìš©: {context}

ì§ˆë¬¸: {question}

ì§€ì¹¨: ë¬¸ì„œ ë‚´ìš©ì—ë§Œ ê¸°ë°˜í•˜ì—¬ ê°„ê²°í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ë‹µë³€:"""
            
            safe_prompt = PromptTemplate(
                template=safe_template,
                input_variables=["context", "question"]
            )
            
            self.conversation_chain = ConversationalRetrievalChain.from_llm(
                llm=self.llm,
                retriever=self.vectorstore.as_retriever(search_kwargs={"k": 2}),  # ê²€ìƒ‰ ê²°ê³¼ ì¤„ì„
                memory=self.memory,
                return_source_documents=True,
                chain_type="stuff",
                combine_docs_chain_kwargs={"prompt": safe_prompt},
                verbose=False  # ë¡œê¹… ë¹„í™œì„±í™”ë¡œ ì•ˆì „ì„± ì¦ëŒ€
            )
            
            return True, len(pages), len(texts)
            
        except Exception as e:
            return False, 0, 0, str(e)
        
        finally:
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(tmp_path)
    
    def chat(self, user_message):
        """ì‚¬ìš©ìì™€ ëŒ€í™”í•˜ê¸°"""
        
        if not self.conversation_chain:
            return "ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”!", [], 0
        
        # ì²« ë²ˆì§¸ ì‹œë„: ì •ì œëœ ì…ë ¥ìœ¼ë¡œ ì‹œë„
        try:
            sanitized_message = sanitize_user_input(user_message)
            start_time = time.time()
            
            response = self.conversation_chain.invoke({"question": sanitized_message})
            
            end_time = time.time()
            response_time = end_time - start_time
            
            answer = response['answer']
            source_docs = response.get('source_documents', [])
            
            return answer, source_docs, response_time
            
        except Exception as e:
            error_msg = str(e).lower()
            
            # Content Filter ì˜¤ë¥˜ ì‹œ ë°±ì—… ì „ëµ ì‹œë„
            if "content filter" in error_msg or "content_filter" in error_msg:
                return self._try_backup_strategy(user_message)
            
            # Rate Limit ì˜¤ë¥˜ ì²˜ë¦¬
            elif "rate limit" in error_msg or "429" in error_msg:
                return (
                    "â³ **ìš”ì²­ í•œë„ ì´ˆê³¼**\n\n"
                    "ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. (ì•½ 1ë¶„ ëŒ€ê¸°)"
                ), [], 0
            
            # ê¸°íƒ€ ì˜¤ë¥˜
            else:
                return f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", [], 0
    
    def _try_backup_strategy(self, original_message):
        """Content Filter ìš°íšŒë¥¼ ìœ„í•œ ë°±ì—… ì „ëµ"""
        
        # ë°±ì—… ì „ëµ 1: ë§¤ìš° ê°„ë‹¨í•œ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜
        simple_queries = [
            "ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”",
            "ì´ ë¬¸ì„œì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ëŠ” ë¬´ì—‡ì¸ê°€ìš”", 
            "ë¬¸ì„œì˜ í•µì‹¬ ì£¼ì œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"
        ]
        
        for simple_query in simple_queries:
            try:
                start_time = time.time()
                
                # ë©”ëª¨ë¦¬ ì„ì‹œ ë¹„í™œì„±í™”ë¡œ ì‹œë„
                temp_chain = ConversationalRetrievalChain.from_llm(
                    llm=self.llm,
                    retriever=self.vectorstore.as_retriever(search_kwargs={"k": 1}),
                    memory=None,  # ë©”ëª¨ë¦¬ ë¹„í™œì„±í™”
                    return_source_documents=True,
                    chain_type="stuff"
                )
                
                response = temp_chain.invoke({"question": simple_query})
                
                end_time = time.time()
                response_time = end_time - start_time
                
                answer = response['answer']
                source_docs = response.get('source_documents', [])
                
                # ì„±ê³µí•œ ê²½ìš° ì•ˆë‚´ ë©”ì‹œì§€ì™€ í•¨ê»˜ ë°˜í™˜
                backup_answer = (
                    f"ğŸ›¡ï¸ **ì•ˆì „ ëª¨ë“œë¡œ ë‹µë³€ë“œë¦½ë‹ˆë‹¤**\n\n"
                    f"ì›ë˜ ì§ˆë¬¸: \"{original_message}\"\n"
                    f"ì•ˆì „ ëª¨ë“œ ë‹µë³€: {answer}\n\n"
                    f"ğŸ’¡ ë” êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì›í•˜ì‹œë©´ ì§ˆë¬¸ì„ ë‹¤ì‹œ í‘œí˜„í•´ë³´ì„¸ìš”."
                )
                
                return backup_answer, source_docs, response_time
                
            except:
                continue
        
        # ëª¨ë“  ë°±ì—… ì „ëµ ì‹¤íŒ¨ ì‹œ
        return (
            "ğŸ›¡ï¸ **ì•ˆì „ í•„í„° ê°ì§€**\n\n"
            "Azure OpenAIì˜ ì•ˆì „ í•„í„°ê°€ ì‘ë™í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒì„ ì‹œë„í•´ë³´ì„¸ìš”:\n\n"
            "1. **ì§ˆë¬¸ì„ ë‹¤ì‹œ í‘œí˜„**í•´ë³´ì„¸ìš”\n"
            "2. **ë” êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì§ˆë¬¸**ì„ í•´ë³´ì„¸ìš”\n"
            "3. **ì „ë¬¸ ìš©ì–´ë‚˜ í•™ìˆ ì  í‘œí˜„**ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”\n\n"
            "ğŸ’¡ ì˜ˆì‹œ:\n"
            "- 'ì´ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”'\n"
            "- '3í˜ì´ì§€ì˜ ì •ì˜ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”'\n"
            "- 'íŠ¹ì • ê°œë…ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”'"
        ), [], 0
    
    def get_chat_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        return self.memory.chat_memory.messages
    
    def clear_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.memory.clear()

def sanitize_user_input(user_input):
    """ì‚¬ìš©ì ì…ë ¥ì„ ì•ˆì „í•˜ê²Œ ì •ì œ (ê°•í™”ëœ ë²„ì „)"""
    
    # ë¯¼ê°í•  ìˆ˜ ìˆëŠ” í‚¤ì›Œë“œë“¤ì„ ì¤‘ì„±ì  í‘œí˜„ìœ¼ë¡œ ë³€ê²½
    replacements = {
        # ê¸°ì¡´ + ì¶”ê°€ í‚¤ì›Œë“œ
        "ìœ„í—˜í•œ": "ì£¼ì˜ê°€ í•„ìš”í•œ",
        "ë¬¸ì œ": "ì´ìŠˆ", 
        "ê³µê²©": "ì ‘ê·¼",
        "í•´í‚¹": "ë³´ì•ˆ ë¶„ì„",
        "í­ë ¥": "ê°•ë ¥í•œ",
        "ì£½ìŒ": "ì¢…ë£Œ",
        "ì‚´ì¸": "ì œê±°",
        "í…ŒëŸ¬": "ê·¹ë‹¨ì  í–‰ìœ„",
        "ì „ìŸ": "ì¶©ëŒ",
        "ì‹¸ì›€": "ê²½ìŸ",
        "íŒŒê´´": "ë³€ê²½",
        "ë²”ì£„": "ê·œì • ìœ„ë°˜",
        "ë¶ˆë²•": "ê·œì •ì— ì–´ê¸‹ë‚˜ëŠ”",
        "í˜ì˜¤": "ë¶€ì •ì ",
        "ì°¨ë³„": "êµ¬ë¶„",
        "ì„±ì ": "ê´€ë ¨ ë‚´ìš©",
        "ì•½ë¬¼": "í™”í•™ ë¬¼ì§ˆ",
        "ë§ˆì•½": "ê·œì œ ë¬¼ì§ˆ",
    }
    
    sanitized = user_input.lower()  # ì†Œë¬¸ìë¡œ ë³€í™˜
    
    # í‚¤ì›Œë“œ êµì²´
    for original, replacement in replacements.items():
        sanitized = sanitized.replace(original, replacement)
    
    # íŠ¹ìˆ˜ ë¬¸ìë‚˜ ì´ëª¨ì§€ ì œê±° (ì•ˆì „ì„± ì¦ëŒ€)
    sanitized = re.sub(r'[^\w\sê°€-í£]', ' ', sanitized)
    
    # ì—°ì†ëœ ê³µë°± ì •ë¦¬
    sanitized = re.sub(r'\s+', ' ', sanitized).strip()
    
    # ë„ˆë¬´ ì§§ì€ ì§ˆë¬¸ì€ êµ¬ì²´í™”
    if len(sanitized) < 5:
        sanitized = f"ë¬¸ì„œì—ì„œ '{sanitized}'ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”"
    
    # í•™ìˆ ì  í‘œí˜„ìœ¼ë¡œ ê°ì‹¸ê¸° (í•­ìƒ ì ìš©)
    academic_prefix = "PDF ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ê°ê´€ì ì´ê³  í•™ìˆ ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”: "
    sanitized = academic_prefix + sanitized
    
    return sanitized

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜ë“¤ ì´ˆê¸°í™”"""
    if 'assistant' not in st.session_state:
        st.session_state.assistant = PDFAssistant()
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    if 'pdf_loaded' not in st.session_state:
        st.session_state.pdf_loaded = False
    if 'pdf_info' not in st.session_state:
        st.session_state.pdf_info = {}

def check_environment():
    """í™˜ê²½ ì„¤ì • í™•ì¸"""
    required_vars = [
        'AZURE_OPENAI_ENDPOINT',
        'AZURE_OPENAI_API_KEY',
        'AZURE_OPENAI_EMBEDDING_DEPLOYMENT',
        'AZURE_OPENAI_CHAT_DEPLOYMENT',
        'AZURE_OPENAI_API_VERSION'
    ]
    
    missing_vars = []
    for var in required_vars:
        if not os.getenv(var):
            missing_vars.append(var)
    
    return missing_vars

def display_chat_message(role, content, timestamp=None, source_docs=None):
    """ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ"""
    with st.chat_message(role):
        if timestamp:
            st.caption(f"ğŸ• {timestamp}")
        st.write(content)
        
        # ì¶œì²˜ ë¬¸ì„œ í‘œì‹œ
        if source_docs and len(source_docs) > 0:
            with st.expander(f"ğŸ“š ì°¸ê³  ë¬¸ì„œ ({len(source_docs)}ê°œ)", expanded=False):
                for i, doc in enumerate(source_docs[:3], 1):  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                    st.markdown(f"**ğŸ“„ {i}ë²ˆì§¸ ì°¸ê³  ë¬¸ì„œ:**")
                    st.text_area(
                        f"ë‚´ìš© {i}",
                        value=doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content,
                        height=100,
                        key=f"source_doc_{i}_{time.time()}"
                    )
                    if doc.metadata:
                        col1, col2 = st.columns(2)
                        with col1:
                            st.caption(f"ğŸ“ í˜ì´ì§€: {doc.metadata.get('page', 'Unknown')}")
                        with col2:
                            st.caption(f"ğŸ“ íŒŒì¼: {doc.metadata.get('source', 'Unknown')}")
                    st.divider()

def display_chat_history():
    """ëŒ€í™” ê¸°ë¡ í‘œì‹œ"""
    if st.session_state.chat_history:
        for chat in st.session_state.chat_history:
            display_chat_message("user", chat['question'], chat.get('timestamp'))
            display_chat_message(
                "assistant", 
                chat['answer'], 
                source_docs=chat.get('source_docs', [])
            )

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    initialize_session_state()
    
    # í—¤ë”
    st.title("ğŸ¤– PDF Assistant with LangChain")
    st.markdown("LangChainì„ ì‚¬ìš©í•œ ë˜‘ë˜‘í•œ PDF ê²€ìƒ‰ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤!")
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("âš™ï¸ Assistant ì„¤ì •")
        
        # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        missing_vars = check_environment()
        if missing_vars:
            st.error("âŒ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤:")
            for var in missing_vars:
                st.write(f"- {var}")
            st.info("ğŸ’¡ .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”!")
            return
        else:
            st.success("âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ")
        
        # PDF ì—…ë¡œë“œ
        st.header("ğŸ“ PDF ì—…ë¡œë“œ")
        uploaded_file = st.file_uploader(
            "PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
            type=['pdf'],
            help="Assistantê°€ í•™ìŠµí•  PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”"
        )
        
        # PDF ì²˜ë¦¬
        if uploaded_file is not None and not st.session_state.pdf_loaded:
            with st.spinner("ğŸ§  Assistantê°€ PDFë¥¼ í•™ìŠµí•˜ëŠ” ì¤‘..."):
                try:
                    result = st.session_state.assistant.load_pdf_knowledge(uploaded_file)
                    
                    if result[0]:  # ì„±ê³µ
                        st.session_state.pdf_loaded = True
                        st.session_state.pdf_info = {
                            'name': uploaded_file.name,
                            'pages': result[1],
                            'chunks': result[2]
                        }
                        st.success("ğŸ‰ PDF í•™ìŠµ ì™„ë£Œ!")
                        st.rerun()
                    else:
                        st.error(f"âŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨: {result[3] if len(result) > 3 else 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}")
                        
                except Exception as e:
                    st.error(f"âŒ PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        # PDF ì •ë³´ í‘œì‹œ
        if st.session_state.pdf_loaded and st.session_state.pdf_info:
            st.header("ğŸ“Š PDF ì •ë³´")
            info = st.session_state.pdf_info
            st.metric("íŒŒì¼ëª…", info['name'])
            st.metric("í˜ì´ì§€ ìˆ˜", info['pages'])
            st.metric("í…ìŠ¤íŠ¸ ì¡°ê°", info['chunks'])
        
        # ëŒ€í™” ê´€ë¦¬
        st.header("ğŸ’¬ ëŒ€í™” ê´€ë¦¬")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("ğŸ”„ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True):
                st.session_state.chat_history = []
                st.session_state.assistant.clear_history()
                st.success("ëŒ€í™” ê¸°ë¡ì„ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤!")
                st.rerun()
        
        with col2:
            if st.button("ğŸ§  ë©”ëª¨ë¦¬ ì •ë¦¬", use_container_width=True):
                # ë©”ëª¨ë¦¬ë§Œ ì •ë¦¬ (í™”ë©´ ê¸°ë¡ì€ ìœ ì§€)
                st.session_state.assistant.clear_history()
                st.success("Assistant ë©”ëª¨ë¦¬ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤!")
                st.info("í™”ë©´ì˜ ëŒ€í™” ê¸°ë¡ì€ ìœ ì§€ë©ë‹ˆë‹¤.")
        
        # í†µê³„ í‘œì‹œ
        if st.button("ğŸ“ˆ í†µê³„ ë³´ê¸°", use_container_width=True):
            if st.session_state.chat_history:
                total_chats = len(st.session_state.chat_history)
                avg_time = sum(chat.get('response_time', 0) for chat in st.session_state.chat_history) / total_chats
                
                # ë©”íŠ¸ë¦­ í‘œì‹œ
                col_a, col_b = st.columns(2)
                with col_a:
                    st.metric("ì´ ëŒ€í™” ìˆ˜", total_chats)
                with col_b:
                    st.metric("í‰ê·  ì‘ë‹µ ì‹œê°„", f"{avg_time:.2f}ì´ˆ")
                
                # Content Filter ê´€ë ¨ í†µê³„
                error_count = sum(1 for chat in st.session_state.chat_history 
                                if "ì•ˆì „ í•„í„°" in chat.get('answer', '') or "ì•ˆì „ ëª¨ë“œ" in chat.get('answer', ''))
                if error_count > 0:
                    st.warning(f"âš ï¸ Content Filter ë°œìƒ: {error_count}íšŒ")
                    st.info("ğŸ’¡ ì§ˆë¬¸ ë°©ì‹ì„ ë” í•™ìˆ ì ìœ¼ë¡œ í‘œí˜„í•´ë³´ì„¸ìš”!")
            else:
                st.info("ì•„ì§ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # Assistant ìƒíƒœ
        st.header("ğŸ¤– Assistant ìƒíƒœ")
        if st.session_state.pdf_loaded:
            st.success("âœ… ì¤€ë¹„ ì™„ë£Œ")
            st.caption("Assistantê°€ PDFë¥¼ í•™ìŠµí–ˆìŠµë‹ˆë‹¤")
        else:
            st.warning("â³ ëŒ€ê¸° ì¤‘")
            st.caption("PDFë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”")
        
        # ì•ˆì „ ê°€ì´ë“œ ì¶”ê°€
        st.header("ğŸ›¡ï¸ ì•ˆì „ ê°€ì´ë“œ")
        with st.expander("Content Filter ê´€ë ¨ íŒ", expanded=False):
            st.markdown("""
            **Azure OpenAI ì•ˆì „ í•„í„°ë¥¼ í”¼í•˜ëŠ” ë°©ë²•:**
            
            âœ… **ì¶”ì²œí•˜ëŠ” ì§ˆë¬¸ ë°©ì‹:**
            - "ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”"
            - "3í˜ì´ì§€ì˜ ì •ì˜ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”" 
            - "ê°œë… Aì™€ Bì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
            - "ì´ë¡ ì  ë°°ê²½ì„ ì•Œë ¤ì£¼ì„¸ìš”"
            
            âŒ **í”¼í•´ì•¼ í•  í‘œí˜„:**
            - ê°ì •ì ì´ê±°ë‚˜ ìê·¹ì ì¸ ë‹¨ì–´
            - ì •ì¹˜ì ì´ê±°ë‚˜ ë¯¼ê°í•œ ì£¼ì œ
            - ë¶€ì •ì ì¸ ë‰˜ì•™ìŠ¤ì˜ í‘œí˜„
            
            ğŸ’¡ **íŒ:**
            - í•™ìˆ ì ì´ê³  ì „ë¬¸ì ì¸ í‘œí˜„ ì‚¬ìš©
            - êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì§ˆë¬¸
            - ì¤‘ë¦½ì ì¸ í†¤ ìœ ì§€
            """)
        
        # ë¬¸ì œ í•´ê²° ê°€ì´ë“œ
        with st.expander("ë¬¸ì œ í•´ê²°", expanded=False):
            st.markdown("""
            **Content Filter ì˜¤ë¥˜ ë°œìƒ ì‹œ:**
            1. ì§ˆë¬¸ì„ ë‹¤ì‹œ í‘œí˜„í•´ë³´ì„¸ìš”
            2. ë” í•™ìˆ ì ì¸ ìš©ì–´ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”
            3. êµ¬ì²´ì ì¸ í˜ì´ì§€ë‚˜ ì„¹ì…˜ì„ ì§€ì •í•´ë³´ì„¸ìš”
            
            **Rate Limit ì˜¤ë¥˜ ë°œìƒ ì‹œ:**
            1. 1-2ë¶„ ê¸°ë‹¤ë¦° í›„ ë‹¤ì‹œ ì‹œë„
            2. ë” ì§§ì€ ì§ˆë¬¸ìœ¼ë¡œ ë‚˜ëˆ ì„œ ë¬¼ì–´ë³´ì„¸ìš”
            """)
    
    # ë©”ì¸ ì˜ì—­
    if st.session_state.pdf_loaded:
        # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
        st.header("ğŸ’¬ Assistantì™€ ëŒ€í™”í•˜ê¸°")
        
        # ì´ì „ ëŒ€í™” ê¸°ë¡ í‘œì‹œ
        display_chat_history()
        
        # ìƒˆë¡œìš´ ì§ˆë¬¸ ì…ë ¥
        if prompt := st.chat_input("Assistantì—ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”!"):
            # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
            timestamp = datetime.now().strftime("%H:%M:%S")
            display_chat_message("user", prompt, timestamp)
            
            # Assistant ì‘ë‹µ ìƒì„±
            with st.chat_message("assistant"):
                with st.spinner("ğŸ¤” Assistantê°€ ìƒê°í•˜ëŠ” ì¤‘..."):
                    try:
                        # Assistantì™€ ëŒ€í™”
                        answer, source_docs, response_time = st.session_state.assistant.chat(prompt)
                        
                        # ë‹µë³€ í‘œì‹œ
                        st.write(answer)
                        st.caption(f"â° ì‘ë‹µ ì‹œê°„: {response_time:.2f}ì´ˆ")
                        
                        # ì¶œì²˜ ë¬¸ì„œ í‘œì‹œ
                        if source_docs:
                            with st.expander(f"ğŸ“š ì°¸ê³  ë¬¸ì„œ ({len(source_docs)}ê°œ)", expanded=False):
                                for i, doc in enumerate(source_docs[:3], 1):
                                    st.markdown(f"**ğŸ“„ {i}ë²ˆì§¸ ì°¸ê³  ë¬¸ì„œ:**")
                                    st.text_area(
                                        f"ë‚´ìš© {i}",
                                        value=doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content,
                                        height=100,
                                        key=f"new_doc_{i}_{time.time()}"
                                    )
                                    if doc.metadata:
                                        col1, col2 = st.columns(2)
                                        with col1:
                                            st.caption(f"ğŸ“ í˜ì´ì§€: {doc.metadata.get('page', 'Unknown')}")
                                        with col2:
                                            st.caption(f"ğŸ“ íŒŒì¼: {doc.metadata.get('source', 'Unknown')}")
                                    st.divider()
                        
                        # ëŒ€í™” ê¸°ë¡ì— ì €ì¥
                        st.session_state.chat_history.append({
                            'question': prompt,
                            'answer': answer,
                            'response_time': response_time,
                            'source_docs': source_docs,
                            'timestamp': timestamp
                        })
                        
                    except Exception as e:
                        st.error(f"âŒ Assistant ì‘ë‹µ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    else:
        # PDFê°€ ì—…ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°
        st.info("ğŸ‘† ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ Assistantë¥¼ í™œì„±í™”í•´ì£¼ì„¸ìš”!")
        
        # ì‚¬ìš©ë²• ì•ˆë‚´
        st.markdown("""
        ### ğŸ¤– PDF Assistant ì‚¬ìš©ë²•
        
        1. **PDF ì—…ë¡œë“œ**: ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ í•™ìŠµì‹œí‚¬ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”
        2. **í•™ìŠµ ëŒ€ê¸°**: Assistantê°€ PDF ë‚´ìš©ì„ ë¶„ì„í•˜ê³  í•™ìŠµí•  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ì„¸ìš”
        3. **ëŒ€í™” ì‹œì‘**: ì±„íŒ…ì°½ì—ì„œ PDF ë‚´ìš©ì— ëŒ€í•´ ììœ ë¡­ê²Œ ì§ˆë¬¸í•˜ì„¸ìš”
        4. **ìŠ¤ë§ˆíŠ¸ ë‹µë³€**: Assistantê°€ PDFì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤
        
        ### âœ¨ ì£¼ìš” ê¸°ëŠ¥
        
        - **ğŸ§  ì§€ëŠ¥í˜• ê²€ìƒ‰**: ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ì´í•´í•˜ì—¬ ê´€ë ¨ ë‚´ìš©ì„ ì •í™•í•˜ê²Œ ì°¾ìŠµë‹ˆë‹¤
        - **ğŸ’¬ ëŒ€í™” ê¸°ì–µ**: ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•˜ì—¬ ë§¥ë½ì— ë§ëŠ” ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤
        - **ğŸ“š ì¶œì²˜ í‘œì‹œ**: ë‹µë³€ì˜ ê·¼ê±°ê°€ ëœ PDF í˜ì´ì§€ì™€ ë‚´ìš©ì„ ë³´ì—¬ì¤ë‹ˆë‹¤
        - **âš¡ ë¹ ë¥¸ ì‘ë‹µ**: ìµœì í™”ëœ ê²€ìƒ‰ìœ¼ë¡œ ë¹ ë¥¸ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤
        
        ### ğŸ’¡ íŒ
        
        - êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ì§ˆë¬¸ì„ í•˜ë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
        - "ì´ì „ì— ë§í•œ ê²ƒì²˜ëŸ¼" ë“±ì˜ í‘œí˜„ìœ¼ë¡œ ì´ì „ ëŒ€í™”ë¥¼ ì°¸ì¡°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
        - ì°¸ê³  ë¬¸ì„œë¥¼ í™•ì¸í•˜ì—¬ ë‹µë³€ì˜ ì •í™•ì„±ì„ ê²€ì¦í•´ë³´ì„¸ìš”
        """)
        
        # ìƒ˜í”Œ ì§ˆë¬¸ ì˜ˆì‹œ
        st.markdown("""
        ### ğŸ“ ìƒ˜í”Œ ì§ˆë¬¸ ì˜ˆì‹œ
        
        - "ì´ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”"
        - "Xì— ëŒ€í•œ ì •ì˜ê°€ ë¬´ì—‡ì¸ê°€ìš”?"
        - "Yì™€ Zì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        - "3í˜ì´ì§€ì—ì„œ ì–¸ê¸‰ëœ ë‚´ìš©ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”"
        """)

if __name__ == "__main__":
    main() 